% === Chapter 01 - Introduction ===
@report{iea2024outlook,
  title={World Energy Outlook 2024},
  author={{International Energy Agency}},
  institution={IEA},
  address={Paris},
  year={2024},
  url={https://www.iea.org/reports/world-energy-outlook-2024},
  note={Licence: CC BY 4.0 (report); CC BY NC SA 4.0 (Annex A)}
}

@misc{iea2024renewable,
  title={Share of renewable electricity generation by technology, 2000-2030},
  author={{International Energy Agency}},
  howpublished={IEA, Paris},
  year={2024},
  url={https://www.iea.org/data-and-statistics/charts/share-of-renewable-electricity-generation-by-technology-2000-2030},
  note={Licence: CC BY 4.0}
}

% === Chapter 02 - Literature Review ===
@article{maghami2016power,
  title     = {Power loss due to soiling on solar panel: A review},
  author    = {Maghami, M. R. and Hizam, H. and Gomes, C. and Radzi, M. A. and Rezadad, M. I. and Hajighorbani, S.},
  journal   = {Renewable and Sustainable Energy Reviews},
  year      = {2016},
  volume    = {59},
  pages     = {1307--1316},
  doi       = {10.1016/j.rser.2016.01.044},
  url       = {https://doi.org/10.1016/j.rser.2016.01.044}
}

@inproceedings{deline2010partially,
  title={Partially shaded operation of multi-string photovoltaic systems},
  author={Deline, Chris},
  booktitle={2010 35th IEEE Photovoltaic Specialists Conference},
  pages={000394--000399},
  year={2010},
  organization={IEEE}
}

@article{liu2015defect,
  title={A defect formation as snail trails in photovoltaic modules},
  author={Liu, Han-Chang and Huang, Chung-Teng and Lee, Wen-Kuei and Yan, Shih-Siang and Lin, Fu-Ming},
  journal={Energy and Power Engineering},
  volume={7},
  number={8},
  pages={348--353},
  year={2015},
  publisher={Scientific Research Publishing}
}

% === Chapter 03 - Methodology ===
@article{vaswani2017attention,
  title     = {Attention is All You Need},
  author    = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N Gomez and Lukasz Kaiser and Illia Polosukhin and Samy Bengio and Yiming Yang},
  journal   = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2017},
  url       = {https://arxiv.org/abs/1706.03762}
}

@article{dosovitskiy2020image,
  title     = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author    = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and others},
  journal   = {International Conference on Learning Representations (ICLR)},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.11929}
}

@article{touvron2021training,
  title     = {Training Data-Efficient Image Transformers \& Distillation through Attention},
  author    = {Hugo Touvron and Matthieu Cord and Matthijs Douze and Francisco Massa and Alexandre Sablayrolles and Herve Jegou},
  journal   = {International Conference on Machine Learning (ICML)},
  year      = {2021},
  url       = {https://arxiv.org/abs/2012.12877}
}

@article{he2016deep,
  title     = {Deep Residual Learning for Image Recognition},
  author    = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  journal   = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2016},
  url       = {https://arxiv.org/abs/1512.03385}
}

@article{Shazeer2017,
   abstract = {The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.},
   author = {Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
   journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
   month = {1},
   publisher = {International Conference on Learning Representations, ICLR},
   title = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
   url = {https://arxiv.org/pdf/1701.06538},
   year = {2017}
}

% === Chapter 04 - Results and Discussion ===
@article{JeffreyKuo2023AutomaticImaging,
    title = {{Automatic detection, classification and localization of defects in large photovoltaic plants using unmanned aerial vehicles (UAV) based infrared (IR) and RGB imaging}},
    year = {2023},
    journal = {Energy Conversion and Management},
    author = {Jeffrey Kuo, Chung Feng and Chen, Sung Hua and Huang, Chao Yang},
    month = {1},
    pages = {116495},
    volume = {276},
    publisher = {Pergamon},
    url = {https://www.sciencedirect.com/science/article/pii/S0196890422012730},
    doi = {10.1016/J.ENCONMAN.2022.116495},
    issn = {0196-8904},
    keywords = {Automatic detection, Photovoltaic plants, Random sample consensus, Scale invariant feature transform, Thermal imager}
}

@article{JEFFREYKUO2023116495,
title = {Automatic detection, classification and localization of defects in large photovoltaic plants using unmanned aerial vehicles (UAV) based infrared (IR) and RGB imaging},
journal = {Energy Conversion and Management},
volume = {276},
pages = {116495},
year = {2023},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2022.116495},
url = {https://www.sciencedirect.com/science/article/pii/S0196890422012730},
author = {Chung-Feng {Jeffrey Kuo} and Sung-Hua Chen and Chao-Yang Huang},
keywords = {Photovoltaic plants, Automatic detection, Thermal imager, Scale invariant feature transform, Random sample consensus},
}

@article{FonsecaAlves2021AutomaticNetworks,
    title = {{Automatic fault classification in photovoltaic modules using Convolutional Neural Networks}},
    year = {2021},
    journal = {Renewable Energy},
    author = {Fonseca Alves, Ricardo Henrique and Deus J{\'{u}}nior, Getúlio Antero de and Marra, Enes Gonçalves and Lemos, Rodrigo Pinto},
    month = {12},
    pages = {502--516},
    volume = {179},
    publisher = {Pergamon},
    url = {https://www.sciencedirect.com/science/article/abs/pii/S0960148121010752?via%3Dihub},
    doi = {10.1016/J.RENENE.2021.07.070},
    issn = {0960-1481},
    keywords = {Convolutional neural network, Data augmentation, Fault classification, Solar energy, Thermography}
}

@article{ibrahim2022machine,
  title={Machine learning schemes for anomaly detection in solar power plants},
  author={Ibrahim, Mariam and Alsheikh, Ahmad and Awaysheh, Feras M and Alshehri, Mohammad Dahman},
  journal={Energies},
  volume={15},
  number={3},
  pages={1082},
  year={2022},
  publisher={MDPI}
}

% === Chapter 05 - Conclusion ===% 
@article{Shazeer2017,
   abstract = {The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.},
   author = {Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
   journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
   month = {1},
   publisher = {International Conference on Learning Representations, ICLR},
   title = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
   url = {https://arxiv.org/pdf/1701.06538},
   year = {2017}
}

@article{Shazeer2017,
   abstract = {The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.},
   author = {Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
   journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
   month = {1},
   publisher = {International Conference on Learning Representations, ICLR},
   title = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
   url = {https://arxiv.org/pdf/1701.06538},
   year = {2017}
}

@article{Shazeer2017,
   abstract = {The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.},
   author = {Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
   journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
   month = {1},
   publisher = {International Conference on Learning Representations, ICLR},
   title = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
   url = {https://arxiv.org/pdf/1701.06538},
   year = {2017}
}

@report{iea2024outlook,
  title={World Energy Outlook 2024},
  author={{International Energy Agency}},
  institution={IEA},
  address={Paris},
  year={2024},
  url={https://www.iea.org/reports/world-energy-outlook-2024},
  note={Licence: CC BY 4.0 (report); CC BY NC SA 4.0 (Annex A)}
}

@misc{iea2024renewable,
  title={Share of renewable electricity generation by technology, 2000-2030},
  author={{International Energy Agency}},
  howpublished={IEA, Paris},
  year={2024},
  url={https://www.iea.org/data-and-statistics/charts/share-of-renewable-electricity-generation-by-technology-2000-2030},
  note={Licence: CC BY 4.0}
}

