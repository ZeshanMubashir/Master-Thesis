According to \cite{vaswani2017attention}, the Transformer model is a neural network
architecture that relies entirely on self-attention mechanisms, discarding
recurrence and convolutions entirely. This architecture has been shown to be highly
effective for various natural language processing tasks, including machine
translation, text summarization, and question answering.